{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dea30fdc-88ab-44ff-b5b5-fc8cd1fe5ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"/home/ubuntu/GNNcoal/\")\n",
    "#from graphseq_inference.data_utils import *\n",
    "from GNNcoal.models import *\n",
    "#from graphseq_inference.train_utils import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c466c9-0ad4-4fe3-b21b-330ae29736b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e884ef7c-6552-4bb0-8fbc-529669e72bdb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './20k_dataset/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./20k_dataset/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m files \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m files \u001b[38;5;241m=\u001b[39m [directory \u001b[38;5;241m+\u001b[39m  file \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m files]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mlen\u001b[39m(files)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './20k_dataset/'"
     ]
    }
   ],
   "source": [
    "directory = \"./20k_dataset/\"\n",
    "\n",
    "files = os.listdir(directory)\n",
    "files = [directory +  file for file in files]\n",
    "\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec762c7-f638-4fa7-abf6-9b4037584c94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36f0e769-127e-47d5-8722-da83e92c07cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_tree_sequence(ts, num_samples):\n",
    "    return ts.simplify(np.random.choice(range(ts.num_samples),\n",
    "                                 num_samples, replace=False).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "585893d5-e632-4990-8925-27f16988667b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tree_sequence_to_data_object_alpha(tree_sequence: tskit.trees.TreeSequence,\n",
    "                                                     parameters: np.ndarray,\n",
    "                                                     num_trees:int = 500,\n",
    "                                                     num_embedding:int = 60, \n",
    "                           ):\n",
    "    \n",
    "    alpha = parameter_set.model\n",
    "    y = torch.Tensor([alpha])\n",
    "    \n",
    "    \n",
    "    \n",
    "    max_num_nodes = 2 * tree_sequence.num_samples - 1 \n",
    "    data_objects = []\n",
    "    \n",
    "    for i, tree in enumerate(tree_sequence.trees()):\n",
    "        if i < num_trees:\n",
    "            \n",
    "            data = from_networkx(nx.Graph(tree.as_dict_of_dicts()))\n",
    "            rename_data_attribute(data, \"branch_length\", \"edge_weight\") \n",
    "            num_nodes = data.num_nodes\n",
    "            data.x = torch.eye(max_num_nodes,num_embedding)\n",
    "            data.x[num_nodes:] = torch.zeros(num_embedding)\n",
    "            data.y = torch.Tensor(y)\n",
    "            data.num_nodes = max_num_nodes\n",
    "            data_objects.append(data)\n",
    "            \n",
    "        else: \n",
    "            break\n",
    "\n",
    "        \n",
    "    return data_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa3d581-514e-4a20-bb03-e068848ea410",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dd486d6c-062e-4e8c-8c10-2ef9d684f996",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = pd.read_csv(\"../20k_seed_0x1337_demographies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cdbf03ea-77b3-4e92-9ce5-429e9075383e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alpha(file): return float(file.split(\"_\")[-1].replace(\".trees\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1238d63f-6f35-46e2-99a5-8258f0df0f45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3217707f-8011-4d40-bdc9-4f579893741a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import basename\n",
    "\n",
    "class AlphaInferenceModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, DemographyNet, time_window=60):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(time_window, time_window//2)\n",
    "        self.l2 = nn.Linear(time_window//2, time_window//4)\n",
    "        self.l3 = nn.Linear(time_window//4, 1)\n",
    "        self.DemographyNet = DemographyNet\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        x = self.DemographyNet(batch)\n",
    "        return self.l3(F.relu(self.l2(F.relu(self.l1(x)))))\n",
    "\n",
    "\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "demography_net = DiffPoolNet(19, 60, 192, 60, track_running_stats=False)\n",
    "\n",
    "model = AlphaInferenceModel(demography_net)\n",
    "model = model.to(device)\n",
    "criterion = RMSELoss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8eb08364-90c3-44b2-9de7-4658248b99dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dd8de2e1-628e-4213-ac47-85dac5058d2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                               | 0/1000000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'beta' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [53]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m nth_scenario \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(file\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m     14\u001b[0m parameter_set \u001b[38;5;241m=\u001b[39m parameters\u001b[38;5;241m.\u001b[39miloc[nth_scenario]\n\u001b[0;32m---> 15\u001b[0m data_objects \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_tree_sequence_to_data_object_alpha\u001b[49m\u001b[43m(\u001b[49m\u001b[43mts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameter_set\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data_objects) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     18\u001b[0m \n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m#mask[population_time <= 10] = False\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m#mask = torch.tile(torch.Tensor(mask), (len(data_objects), 1))\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "Input \u001b[0;32mIn [24]\u001b[0m, in \u001b[0;36mconvert_tree_sequence_to_data_object_alpha\u001b[0;34m(tree_sequence, parameters, num_trees, num_embedding)\u001b[0m\n\u001b[1;32m     10\u001b[0m ts \u001b[38;5;241m=\u001b[39m tree_sequence\n\u001b[1;32m     11\u001b[0m ts \u001b[38;5;241m=\u001b[39m msprime\u001b[38;5;241m.\u001b[39mmutate(ts, \u001b[38;5;241m1e-8\u001b[39m)\n\u001b[0;32m---> 12\u001b[0m ne \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_beta_coal_ne_estimate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_mutations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mts\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msequence_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1e-8\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m max_num_nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m tree_sequence\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m \n\u001b[1;32m     16\u001b[0m data_objects \u001b[38;5;241m=\u001b[39m []\n",
      "Input \u001b[0;32mIn [45]\u001b[0m, in \u001b[0;36mcalculate_beta_coal_ne_estimate\u001b[0;34m(theta, sample_size, L, alpha, mu_real)\u001b[0m\n\u001b[1;32m      8\u001b[0m mu_estimated \u001b[38;5;241m=\u001b[39m theta \u001b[38;5;241m/\u001b[39m ((\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39mnp\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, sample_size)))) \u001b[38;5;241m*\u001b[39m L)\n\u001b[1;32m      9\u001b[0m m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m((\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(alpha\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m*\u001b[39m(alpha\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)))\n\u001b[0;32m---> 10\u001b[0m scale \u001b[38;5;241m=\u001b[39m (m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39malpha)\u001b[38;5;241m/\u001b[39m(alpha \u001b[38;5;241m*\u001b[39m \u001b[43mbeta\u001b[49m(\u001b[38;5;241m2\u001b[39m\u001b[38;5;241m-\u001b[39malpha,alpha))\n\u001b[1;32m     11\u001b[0m Ne \u001b[38;5;241m=\u001b[39m (((mu_estimated\u001b[38;5;241m/\u001b[39mmu_real)\u001b[38;5;241m/\u001b[39mscale)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m(\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m/\u001b[39m(alpha\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)))\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Ne\n",
      "\u001b[0;31mNameError\u001b[0m: name 'beta' is not defined"
     ]
    }
   ],
   "source": [
    "length = 1\n",
    "loss_all = []\n",
    "\n",
    "for epoch in range(0, 10):\n",
    "    np.random.shuffle(files)\n",
    "    for i, file in enumerate(tqdm(files)):\n",
    "\n",
    "\n",
    "        ts, mask = torch.load(file)\n",
    "        #ts = tskit.load(file)\n",
    "        #ts = tskit.load(file)\n",
    "        #data_objects = convert_tree_sequence_to_data_object_alpha(ts, get_alpha(file))\n",
    "        nth_scenario = int(file.split(\"_\")[2])\n",
    "        parameter_set = parameters.iloc[nth_scenario]\n",
    "        data_objects = convert_tree_sequence_to_data_object_alpha(ts, parameter_set)\n",
    "\n",
    "        if len(data_objects) > 1:\n",
    "        \n",
    "            #mask[population_time <= 10] = False\n",
    "            #mask = torch.tile(torch.Tensor(mask), (len(data_objects), 1))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            dl = DataLoader(data_objects, batch_size=len(data_objects))\n",
    "            for batch in dl:\n",
    "                batch = batch.to(device)\n",
    "                y_hat = model(batch)\n",
    "                y_true = data_objects[0].y.tile(len(data_objects)).reshape(len(data_objects), length).to(device)\n",
    "\n",
    "                loss = criterion(y_hat, y_true) \n",
    "                loss.backward()\n",
    "                loss_all.append(loss.item())\n",
    "                optimizer.step()\n",
    "\n",
    "            if i != 0 and i % 10000 == 0:\n",
    "                loss_all = np.mean(loss_all)\n",
    "                print(f\"loss {loss_all}\")\n",
    "                torch.save(model.state_dict(), \"./alpha_inf/mmc_diffpool_model_alpha_inference_intermediate\" + str(epoch) + \"_\" + str(i) + \".pth\")\n",
    "                os.system(f'echo \"Epoch: {epoch:03d}, Train Loss: {np.mean(loss_all):.4f}\" >> ./alpha_inf/mmc_diffpool_model_alpha_inference_intermediate.txt')\n",
    "                loss_all = []\n",
    "                \n",
    "torch.save(model.state_dict(), \"./alpha_inf/mmc_diffpool_model_alpha_inference_intermediate\" + str(epoch) + \"_\" + str(i) + \".pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6662c71c-d84d-4e37-87cf-8c8e53b5ee50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8428f0ac-3f29-4a1e-82d9-fd2805694422",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecfd7bd-ca76-4a44-ba63-4f9037427ff1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0ffe54-2ccc-4ed6-95d8-f6c891c06eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fb143b-13e8-4eaf-9dc5-7f9d2d017956",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "60d20510-a547-490c-a158-296ccc3f9e4a",
   "metadata": {},
   "source": [
    "with scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "6d028154-a39a-46af-95e1-3c5e38251453",
   "metadata": {},
   "outputs": [],
   "source": [
    "#theta = 10000\n",
    "#sample_size = 4\n",
    "#L = 1_000_000\n",
    "#alpha = 1.9\n",
    "#mu_real = 1e-9\n",
    "\n",
    "def calculate_beta_coal_ne_estimate(theta, sample_size, L, alpha, mu_real):\n",
    "    mu_estimated = theta / ((2*np.sum(1/np.array(range(1, sample_size)))) * L)\n",
    "    m = 1 + (1/((2**(alpha-1))*(alpha-1)))\n",
    "    scale = (m**alpha)/(alpha * beta(2-alpha,alpha))\n",
    "    Ne = (((mu_estimated/mu_real)/scale)**(1/(alpha-1)))\n",
    "    return Ne\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "b845999e-6eb0-47ad-85ff-ac647b75a612",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "338a84f4-c3d4-4352-879f-b4f50152247a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000000"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "directory = \"../../dataset/20k_dataset/\"\n",
    "\n",
    "files = os.listdir(directory)\n",
    "files = [directory +  file for file in files]\n",
    "\n",
    "len(files)\n",
    "\n",
    "directory = \"../../dataset/10k_dataset2/\"\n",
    "\n",
    "files2 = os.listdir(directory)\n",
    "files2 = [directory +  file for file in files2]\n",
    "\n",
    "len(files + files2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630833a0-e978-4904-a9bb-d76d0148fd39",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "c669d842-e4f4-45ee-9745-d9b2ba6fc3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tree_sequence_to_data_object_alpha(tree_sequence: tskit.trees.TreeSequence,\n",
    "                                                     parameters: np.ndarray,\n",
    "                                                     num_trees:int = 500,\n",
    "                                                     num_embedding:int = 60, \n",
    "                           ):\n",
    "    \n",
    "    alpha = parameter_set.model\n",
    "    y = torch.Tensor([alpha])\n",
    "    \n",
    "    ts = tree_sequence\n",
    "    ts = msprime.mutate(ts, 1e-8)\n",
    "    #print(ts.num_mutations, ts.sample_size, ts.sequence_length, parameters.model, 1e-8)\n",
    "    ne = calculate_beta_coal_ne_estimate(ts.num_mutations, ts.sample_size, ts.sequence_length, parameters.model, 1e-8)\n",
    "    \n",
    "    max_num_nodes = 2 * tree_sequence.num_samples - 1 \n",
    "    data_objects = []\n",
    "    \n",
    "    for i, tree in enumerate(tree_sequence.trees()):\n",
    "        if i < num_trees:\n",
    "            \n",
    "            data = from_networkx(nx.Graph(tree.as_dict_of_dicts()))\n",
    "            rename_data_attribute(data, \"branch_length\", \"edge_weight\") \n",
    "            num_nodes = data.num_nodes\n",
    "            data.x = torch.eye(max_num_nodes,num_embedding)\n",
    "            data.x[num_nodes:] = torch.zeros(num_embedding)\n",
    "            data.y = torch.Tensor(y)\n",
    "            data.num_nodes = max_num_nodes\n",
    "            \n",
    "            #print(data.edge_weight)\n",
    "            data.edge_weight = np.log(data.edge_weight) / np.log(ne)\n",
    "            #print(data.edge_weight)\n",
    "            data_objects.append(data)\n",
    "            \n",
    "            \n",
    "\n",
    "            \n",
    "        else: \n",
    "            break\n",
    "\n",
    "        \n",
    "    return data_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "95e4197f-2a78-4861-82cb-6a8db9a3f9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import basename\n",
    "\n",
    "class AlphaInferenceModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, DemographyNet, time_window=60):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(time_window, time_window//2)\n",
    "        self.l2 = nn.Linear(time_window//2, time_window//4)\n",
    "        self.l3 = nn.Linear(time_window//4, 1)\n",
    "        self.DemographyNet = DemographyNet\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        x = self.DemographyNet(batch)\n",
    "        return self.l3(F.relu(self.l2(F.relu(self.l1(x)))))\n",
    "\n",
    "\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "demography_net = DiffPoolNet(19, 60, 192, 60, track_running_stats=False)\n",
    "\n",
    "model = AlphaInferenceModel(demography_net)\n",
    "model = model.to(device)\n",
    "criterion = RMSELoss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "379f9e87-e5a2-4257-913a-9d77117a69ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘trained_models/scaled_alpha_inf’: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "mkdir trained_models/scaled_alpha_inf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83e2ce0-1cfb-4f82-bf91-b5a09fa5962a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "1499ca25-b462-4fee-9cc9-7573d4fa5c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                | 1001/1000000 [08:20<146:13:09,  1.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.1872798585026831\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                | 2001/1000000 [16:39<125:56:46,  2.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.07131471575982869\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                | 3001/1000000 [25:09<176:30:22,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.0649269547360018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|▏                               | 4001/1000000 [33:45<140:37:07,  1.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.05812267012055963\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏                               | 5001/1000000 [42:17<131:29:19,  2.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.056721819828264415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏                               | 6001/1000000 [50:41<155:24:22,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.062480737618170676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏                               | 7001/1000000 [59:55<138:24:44,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.05537466559931636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏                             | 7929/1000000 [1:09:06<336:01:29,  1.22s/it]/tmp/ipykernel_317923/3806840690.py:30: RuntimeWarning: divide by zero encountered in log\n",
      "  data.edge_weight = np.log(data.edge_weight) / np.log(ne)\n",
      "  1%|▏                             | 8001/1000000 [1:09:41<126:32:30,  2.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.050738472274287384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▎                             | 9001/1000000 [1:19:04<135:21:30,  2.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.054924420029856265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▎                            | 10001/1000000 [1:28:14<143:23:10,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.05213312864489853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▎                            | 11001/1000000 [1:37:11<127:56:03,  2.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.0525734677856734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▎                            | 12001/1000000 [1:46:28<146:29:04,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.04884038133174181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                            | 13001/1000000 [1:59:32<892:32:05,  3.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.05020697299391031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▍                            | 13487/1000000 [2:10:39<159:17:19,  1.72it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/dl13/lib/python3.9/site-packages/networkx/classes/digraph.py:482\u001b[0m, in \u001b[0;36mDiGraph.add_nodes_from\u001b[0;34m(self, nodes_for_adding, **attr)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 482\u001b[0m     newnode \u001b[38;5;241m=\u001b[39m \u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_node\u001b[49m\n\u001b[1;32m    483\u001b[0m     newdict \u001b[38;5;241m=\u001b[39m attr\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'dict'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [106]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m nth_scenario \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(file\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m2\u001b[39m])\n\u001b[1;32m     14\u001b[0m parameter_set \u001b[38;5;241m=\u001b[39m parameters\u001b[38;5;241m.\u001b[39miloc[nth_scenario]\n\u001b[0;32m---> 15\u001b[0m data_objects \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_tree_sequence_to_data_object_alpha\u001b[49m\u001b[43m(\u001b[49m\u001b[43mts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameter_set\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data_objects) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m     18\u001b[0m \n\u001b[1;32m     19\u001b[0m     \u001b[38;5;66;03m#mask[population_time <= 10] = False\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;66;03m#mask = torch.tile(torch.Tensor(mask), (len(data_objects), 1))\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "Input \u001b[0;32mIn [104]\u001b[0m, in \u001b[0;36mconvert_tree_sequence_to_data_object_alpha\u001b[0;34m(tree_sequence, parameters, num_trees, num_embedding)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, tree \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(tree_sequence\u001b[38;5;241m.\u001b[39mtrees()):\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m<\u001b[39m num_trees:\n\u001b[0;32m---> 21\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43mfrom_networkx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mGraph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_dict_of_dicts\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m         rename_data_attribute(data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbranch_length\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medge_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m) \n\u001b[1;32m     23\u001b[0m         num_nodes \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mnum_nodes\n",
      "File \u001b[0;32m~/miniconda3/envs/dl13/lib/python3.9/site-packages/torch_geometric/utils/convert.py:142\u001b[0m, in \u001b[0;36mfrom_networkx\u001b[0;34m(G, group_node_attrs, group_edge_attrs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnetworkx\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnx\u001b[39;00m\n\u001b[1;32m    141\u001b[0m G \u001b[38;5;241m=\u001b[39m nx\u001b[38;5;241m.\u001b[39mconvert_node_labels_to_integers(G)\n\u001b[0;32m--> 142\u001b[0m G \u001b[38;5;241m=\u001b[39m \u001b[43mG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_directed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m nx\u001b[38;5;241m.\u001b[39mis_directed(G) \u001b[38;5;28;01melse\u001b[39;00m G\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(G, (nx\u001b[38;5;241m.\u001b[39mMultiGraph, nx\u001b[38;5;241m.\u001b[39mMultiDiGraph)):\n\u001b[1;32m    145\u001b[0m     edges \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(G\u001b[38;5;241m.\u001b[39medges(keys\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "File \u001b[0;32m~/miniconda3/envs/dl13/lib/python3.9/site-packages/networkx/classes/graph.py:1596\u001b[0m, in \u001b[0;36mGraph.to_directed\u001b[0;34m(self, as_view)\u001b[0m\n\u001b[1;32m   1594\u001b[0m G \u001b[38;5;241m=\u001b[39m graph_class()\n\u001b[1;32m   1595\u001b[0m G\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mupdate(deepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgraph))\n\u001b[0;32m-> 1596\u001b[0m \u001b[43mG\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_nodes_from\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43md\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_node\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1597\u001b[0m G\u001b[38;5;241m.\u001b[39madd_edges_from(\n\u001b[1;32m   1598\u001b[0m     (u, v, deepcopy(data))\n\u001b[1;32m   1599\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m u, nbrs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adj\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   1600\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m v, data \u001b[38;5;129;01min\u001b[39;00m nbrs\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   1601\u001b[0m )\n\u001b[1;32m   1602\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m G\n",
      "File \u001b[0;32m~/miniconda3/envs/dl13/lib/python3.9/site-packages/networkx/classes/digraph.py:482\u001b[0m, in \u001b[0;36mDiGraph.add_nodes_from\u001b[0;34m(self, nodes_for_adding, **attr)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m nodes_for_adding:\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 482\u001b[0m         newnode \u001b[38;5;241m=\u001b[39m \u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_node\u001b[49m\n\u001b[1;32m    483\u001b[0m         newdict \u001b[38;5;241m=\u001b[39m attr\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "length = 1\n",
    "loss_all = []\n",
    "\n",
    "for epoch in range(0, 10):\n",
    "    np.random.shuffle(files)\n",
    "    for i, file in enumerate(tqdm(files)):\n",
    "\n",
    "\n",
    "        ts, mask = torch.load(file)\n",
    "        #ts = tskit.load(file)\n",
    "        #ts = tskit.load(file)\n",
    "        #data_objects = convert_tree_sequence_to_data_object_alpha(ts, get_alpha(file))\n",
    "        nth_scenario = int(file.split(\"_\")[2])\n",
    "        parameter_set = parameters.iloc[nth_scenario]\n",
    "        data_objects = convert_tree_sequence_to_data_object_alpha(ts, parameter_set)\n",
    "\n",
    "        if len(data_objects) > 1:\n",
    "        \n",
    "            #mask[population_time <= 10] = False\n",
    "            #mask = torch.tile(torch.Tensor(mask), (len(data_objects), 1))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            dl = DataLoader(data_objects, batch_size=len(data_objects))\n",
    "            for batch in dl:\n",
    "                batch = batch.to(device)\n",
    "                y_hat = model(batch)\n",
    "                y_true = data_objects[0].y.tile(len(data_objects)).reshape(len(data_objects), length).to(device)\n",
    "\n",
    "                loss = criterion(y_hat, y_true) \n",
    "                loss.backward()\n",
    "                loss_all.append(loss.item())\n",
    "                optimizer.step()\n",
    "\n",
    "            if i != 0 and i % 1000 == 0:\n",
    "                loss_all = np.mean(loss_all)\n",
    "                print(f\"loss {loss_all}\")\n",
    "                torch.save(model.state_dict(), \"../trained_models/scaled_alpha_inf/mmc_diffpool_model_alpha_inference_intermediate\" + str(epoch) + \"_\" + str(i) + \".pth\")\n",
    "                os.system(f'echo \"Epoch: {epoch:03d}, Train Loss: {np.mean(loss_all):.4f}\" >> ../trained_models/scaled_alpha_inf/mmc_diffpool_model_alpha_inference_intermediate.txt')\n",
    "                loss_all = []\n",
    "                \n",
    "torch.save(model.state_dict(), \"../trained_models/scaled_alpha_inf/mmc_diffpool_model_alpha_inference_intermediate\" + str(epoch) + \"_\" + str(i) + \".pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "556036ce-bfba-4855-a6d5-0761d0cd301b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1.9200], device='cuda:0'),\n",
       " tensor([1.9289], device='cuda:0', grad_fn=<SelectBackward0>))"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true[0], y_hat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426e6cb7-3460-4d2f-ad4e-df74a6cdd3f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7da60aa-3a15-4dfe-87b8-bb5c4517a100",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
