{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dea30fdc-88ab-44ff-b5b5-fc8cd1fe5ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"/home/ubuntu/graphseq-inference/\")\n",
    "from graphseq_inference.data_utils import *\n",
    "from graphseq_inference.models import *\n",
    "from graphseq_inference.train_utils import *\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c466c9-0ad4-4fe3-b21b-330ae29736b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e884ef7c-6552-4bb0-8fbc-529669e72bdb",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './20k_dataset/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [6]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m directory \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./20k_dataset/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m files \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdirectory\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m files \u001b[38;5;241m=\u001b[39m [directory \u001b[38;5;241m+\u001b[39m  file \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m files]\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mlen\u001b[39m(files)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './20k_dataset/'"
     ]
    }
   ],
   "source": [
    "directory = \"./20k_dataset/\"\n",
    "\n",
    "files = os.listdir(directory)\n",
    "files = [directory +  file for file in files]\n",
    "\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec762c7-f638-4fa7-abf6-9b4037584c94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36f0e769-127e-47d5-8722-da83e92c07cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_tree_sequence(ts, num_samples):\n",
    "    return ts.simplify(np.random.choice(range(ts.num_samples),\n",
    "                                 num_samples, replace=False).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "585893d5-e632-4990-8925-27f16988667b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tree_sequence_to_data_object_alpha(tree_sequence: tskit.trees.TreeSequence,\n",
    "                                                     parameters: np.ndarray,\n",
    "                                                     num_trees:int = 500,\n",
    "                                                     num_embedding:int = 60, \n",
    "                           ):\n",
    "    \n",
    "    alpha = parameter_set.model\n",
    "    y = torch.Tensor([alpha])\n",
    "    \n",
    "    \n",
    "    \n",
    "    max_num_nodes = 2 * tree_sequence.num_samples - 1 \n",
    "    data_objects = []\n",
    "    \n",
    "    for i, tree in enumerate(tree_sequence.trees()):\n",
    "        if i < num_trees:\n",
    "            \n",
    "            data = from_networkx(nx.Graph(tree.as_dict_of_dicts()))\n",
    "            rename_data_attribute(data, \"branch_length\", \"edge_weight\") \n",
    "            num_nodes = data.num_nodes\n",
    "            data.x = torch.eye(max_num_nodes,num_embedding)\n",
    "            data.x[num_nodes:] = torch.zeros(num_embedding)\n",
    "            data.y = torch.Tensor(y)\n",
    "            data.num_nodes = max_num_nodes\n",
    "            data_objects.append(data)\n",
    "            \n",
    "        else: \n",
    "            break\n",
    "\n",
    "        \n",
    "    return data_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56ebd675-6d24-44aa-8f0a-0d35dd7fe8ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa3d581-514e-4a20-bb03-e068848ea410",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dd486d6c-062e-4e8c-8c10-2ef9d684f996",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = pd.read_csv(\"20k_seed_0x1337_demographies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261e40e2-e241-418a-a26c-c032443ef511",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cdbf03ea-77b3-4e92-9ce5-429e9075383e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alpha(file): return float(file.split(\"_\")[-1].replace(\".trees\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1238d63f-6f35-46e2-99a5-8258f0df0f45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3217707f-8011-4d40-bdc9-4f579893741a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import basename\n",
    "\n",
    "class AlphaInferenceModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, DemographyNet, time_window=60):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(time_window, time_window//2)\n",
    "        self.l2 = nn.Linear(time_window//2, time_window//4)\n",
    "        self.l3 = nn.Linear(time_window//4, 1)\n",
    "        self.DemographyNet = DemographyNet\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        x = self.DemographyNet(batch)\n",
    "        return self.l3(F.relu(self.l2(F.relu(self.l1(x)))))\n",
    "\n",
    "\n",
    "    \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "demography_net = DiffPoolNet(19, 60, 192, 60, track_running_stats=False)\n",
    "\n",
    "model = AlphaInferenceModel(demography_net)\n",
    "model = model.to(device)\n",
    "criterion = RMSELoss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cabcca39-4c3a-4854-986b-5075aaa1b855",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "16267581-38f4-409c-8790-7a27d74673e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preorder_dist(tree):\n",
    "    result = []\n",
    "    for root in tree.roots:\n",
    "        stack = [(root, (-1, root) , (tree.get_time(tree.root), 0))]\n",
    "        while len(stack) > 0:\n",
    "            u, pc, time = stack.pop()\n",
    "            result.append((u, pc, time))\n",
    "            for v in tree.children(u):\n",
    "                stack.append((v, (u,v) , (tree.get_time(u),tree.get_time(v))))\n",
    "    return result\n",
    "\n",
    "\n",
    "def multiple_mergerized_to_data_object(result):\n",
    "\n",
    "    G = nx.Graph()\n",
    "    for _, edge, times in result[:-1]:\n",
    "        a, b = edge\n",
    "        ta, tb = times\n",
    "        branch = ta - tb\n",
    "        G.add_weighted_edges_from([(a, b, branch)])\n",
    "        \n",
    "    data = from_networkx(G)\n",
    "    rename_data_attribute(data, \"weight\", \"edge_weight\") \n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def add_x(data, num_embedding = 60):\n",
    "    max_num_nodes = 2 * 10 - 1 \n",
    "    num_nodes = data.num_nodes\n",
    "    data.x = torch.eye(max_num_nodes,num_embedding)\n",
    "    data.x[num_nodes:] = torch.zeros(num_embedding)\n",
    "    data.num_nodes = max_num_nodes\n",
    "    return data\n",
    "\n",
    "def ts_to_data_objects(ts):\n",
    "    \n",
    "    data_objects = []\n",
    "    for tree in ts.trees():\n",
    "        result = preorder_dist(tree)\n",
    "        result.reverse()\n",
    "        result = restructure_result(result, threshold=10)    \n",
    "        data = multiple_mergerized_to_data_object(result)\n",
    "        data_objects.append(data)  \n",
    "        \n",
    "    return data_objects\n",
    "\n",
    "\n",
    "\n",
    "def restructure_result(result, threshold = 2):\n",
    "\n",
    "    is_modified = True\n",
    "    while True:\n",
    "\n",
    "        if not is_modified: break\n",
    "        is_modified = False\n",
    "\n",
    "        for i, (idx_node, (parent, node), (time_parent, time_node)) in enumerate(result):\n",
    "            branch_length = time_parent - time_node            \n",
    "\n",
    "            threshold = 20\n",
    "            \n",
    "            if time_parent < 2000:\n",
    "                threshold = 200\n",
    "            \n",
    "            if time_parent < 200:\n",
    "                threshold = 20\n",
    "            \n",
    "            if time_parent < 20:\n",
    "                threshold = 2\n",
    "                \n",
    "            if time_parent < 10:\n",
    "                threshold = 1\n",
    "                \n",
    "            if time_parent < 5:\n",
    "                threshold = 0.5\n",
    "                \n",
    "            \n",
    "            if branch_length < threshold and branch_length != 0 and (parent != node):\n",
    "    \n",
    "                \n",
    "                new_time = (time_parent+time_node)/2\n",
    "                result[i] = (-1, (parent, parent), (time_parent, time_parent))\n",
    "                for j, (_, (p, n), (tp, tn)) in enumerate(result):\n",
    "                    if node == n:\n",
    "                        result[j] = (-1, (p, parent), (tp, time_parent))#\n",
    "                    if node == p:\n",
    "                        result[j] = (-1, (parent, n), (time_parent, tn))\n",
    "\n",
    "                is_modified = True\n",
    "                break\n",
    "\n",
    "    new_result = []\n",
    "    for a, (b,c), (d, e) in result:\n",
    "        if b != c:\n",
    "            new_result.append((a, (b,c), (d,e)))\n",
    "    return new_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38014c72-0ec7-4d47-a966-f815c0c2ce1a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c0f3fdf5-049a-4dfe-b50d-3a77926494b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9901b570-1d28-404d-be01-cb34557fac99",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "8eb08364-90c3-44b2-9de7-4658248b99dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd8de2e1-628e-4213-ac47-85dac5058d2d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                 | 1200/1000000 [08:06<92:48:05,  2.99it/s]"
     ]
    }
   ],
   "source": [
    "length = 1\n",
    "loss_all = []\n",
    "\n",
    "for epoch in range(0, 10):\n",
    "    np.random.shuffle(files)\n",
    "    for i, file in enumerate(tqdm(files)):\n",
    "\n",
    "\n",
    "        ts, mask = torch.load(file)\n",
    "        #ts = tskit.load(file)\n",
    "        #ts = tskit.load(file)\n",
    "        #data_objects = convert_tree_sequence_to_data_object_alpha(ts, get_alpha(file))\n",
    "        nth_scenario = int(file.split(\"_\")[2])\n",
    "        parameter_set = parameters.iloc[nth_scenario]\n",
    "        data_objects = convert_tree_sequence_to_data_object_alpha(ts, parameter_set)\n",
    "\n",
    "        if len(data_objects) > 1:\n",
    "        \n",
    "            #mask[population_time <= 10] = False\n",
    "            #mask = torch.tile(torch.Tensor(mask), (len(data_objects), 1))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            dl = DataLoader(data_objects, batch_size=len(data_objects))\n",
    "            for batch in dl:\n",
    "                batch = batch.to(device)\n",
    "                y_hat = model(batch)\n",
    "                y_true = data_objects[0].y.tile(len(data_objects)).reshape(len(data_objects), length).to(device)\n",
    "\n",
    "                loss = criterion(y_hat, y_true) \n",
    "                loss.backward()\n",
    "                loss_all.append(loss.item())\n",
    "                optimizer.step()\n",
    "\n",
    "            if i != 0 and i % 10000 == 0:\n",
    "                loss_all = np.mean(loss_all)\n",
    "                print(f\"loss {loss_all}\")\n",
    "                torch.save(model.state_dict(), \"./alpha_inf/mmc_diffpool_model_alpha_inference_intermediate\" + str(epoch) + \"_\" + str(i) + \".pth\")\n",
    "                os.system(f'echo \"Epoch: {epoch:03d}, Train Loss: {np.mean(loss_all):.4f}\" >> ./alpha_inf/mmc_diffpool_model_alpha_inference_intermediate.txt')\n",
    "                loss_all = []\n",
    "                \n",
    "torch.save(model.state_dict(), \"./alpha_inf/mmc_diffpool_model_alpha_inference_intermediate\" + str(epoch) + \"_\" + str(i) + \".pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8266d5-9b9e-4579-821a-84853fbb660e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2d0496-6493-4911-973e-6313b25dcac3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a802c8fc-a1ba-48f9-987f-f9e30e688bd1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4339f9-f68d-4a2c-8d2d-e088587d2191",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa9c3c8-2c9e-4d37-bac7-350784880cdc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3f8b271-7472-4e5d-90fc-9d91b82d4a2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae844c41-0b79-4d53-9a4d-a4021045911a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91f8f87-f05c-4f7d-80b3-99babe3e6304",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f29ad9f-2736-469b-8f66-3419d38b6058",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3635df-e231-4e18-8572-edf5e145663c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ca5d93-225a-4891-81ec-99b53e3a23d4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4739747f-4352-4da2-9431-3f4f75db4e30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a356f26-a7cb-413d-9893-e8e9a15e094e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d03fd9-b46b-4abd-8382-98aeab9ee597",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88df82e-f573-40d9-99eb-a42eb70f4908",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d46fee-8102-4e93-b5aa-b1f61d3b4aef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
