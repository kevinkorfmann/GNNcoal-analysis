{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "weighted-calgary",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"/home/anon/projects2/GNNcoal/\")\n",
    "#from graphseq_inference.data_utils import *\n",
    "from GNNcoal.models import *\n",
    "#from graphseq_inference.train_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "violent-wallet",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tskit\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "first-seating",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './csvs/20k_dataset/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_27008/24000238.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"./csvs/20k_dataset/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirectory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mfiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mdirectory\u001b[0m \u001b[0;34m+\u001b[0m  \u001b[0mfile\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './csvs/20k_dataset/'"
     ]
    }
   ],
   "source": [
    "directory = \"./csvs/20k_dataset/\"\n",
    "\n",
    "files = os.listdir(directory)\n",
    "files = [directory +  file for file in files]\n",
    "\n",
    "\n",
    "len(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respected-vertex",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "young-bahrain",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "organic-beginning",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tree_sequence_to_data_object_alpha_r(tree_sequence: tskit.trees.TreeSequence,\n",
    "                                                     parameters: np.ndarray,\n",
    "                                                     num_trees:int = 500,\n",
    "                                                     num_embedding:int = 60, \n",
    "                           ):\n",
    "    \n",
    "    alpha = parameter_set.model\n",
    "    y = torch.Tensor([alpha])\n",
    "    \n",
    "    tree_sequence = reduce_tree_sequence(tree_sequence, 3)\n",
    "\n",
    "    \n",
    "    max_num_nodes = 2 * tree_sequence.num_samples - 1 \n",
    "    data_objects = []\n",
    "    \n",
    "    for i, tree in enumerate(tree_sequence.trees()):\n",
    "        if i < num_trees:\n",
    "            \n",
    "            data = from_networkx(nx.Graph(tree.as_dict_of_dicts()))\n",
    "            rename_data_attribute(data, \"branch_length\", \"edge_weight\") \n",
    "            num_nodes = data.num_nodes\n",
    "            data.x = torch.eye(max_num_nodes,num_embedding)\n",
    "            data.x[num_nodes:] = torch.zeros(num_embedding)\n",
    "            data.y = torch.Tensor(y)\n",
    "            data.num_nodes = max_num_nodes\n",
    "            data_objects.append(data)\n",
    "            \n",
    "        else: \n",
    "            break\n",
    "\n",
    "        \n",
    "    return data_objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "correct-budget",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "spare-conclusion",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bacterial-jewelry",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/anon/projects2/GNNcoal-analysis/training_notebooks'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "infrared-physics",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = pd.read_csv(\"../csvs/20k_seed_0x1337_demographies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "burning-paper",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alpha(file): return float(file.split(\"_\")[-1].replace(\".trees\", \"\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honest-branch",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-emerald",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outside-prior",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extraordinary-cricket",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parallel-bracket",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geological-mitchell",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "satellite-margin",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "neutral-worthy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import basename\n",
    "\n",
    "\n",
    "class AlphaInferenceModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, DemographyNet, time_window=60):\n",
    "        super().__init__()\n",
    "        self.l1 = nn.Linear(time_window, time_window//2)\n",
    "        self.l2 = nn.Linear(time_window//2, time_window//4)\n",
    "        self.l3 = nn.Linear(time_window//4, 1)\n",
    "        self.DemographyNet = DemographyNet\n",
    "    \n",
    "    def forward(self, batch):\n",
    "        x = self.DemographyNet(batch)\n",
    "        return self.l3(F.relu(self.l2(F.relu(self.l1(x)))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "heavy-comparative",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSELoss(yhat,y):\n",
    "    return torch.sqrt(torch.mean((yhat-y)**2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "union-norway",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "demography_net = DiffPoolNet(19, 60, 192, 60, track_running_stats=False)\n",
    "model = AlphaInferenceModel(demography_net)\n",
    "model = model.to(device)\n",
    "criterion = RMSELoss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acute-ordinary",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "concerned-partner",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "metallic-intermediate",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "increasing-review",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "treated-pattern",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smart-asian",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deluxe-spirituality",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "amended-distribution",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'files' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_27008/896441780.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'files' is not defined"
     ]
    }
   ],
   "source": [
    "length = 1\n",
    "loss_all = []\n",
    "\n",
    "for epoch in range(0, 10):\n",
    "    np.random.shuffle(files)\n",
    "    for i, file in enumerate(tqdm(files)):\n",
    "\n",
    "\n",
    "        ts, mask = torch.load(file)\n",
    "        #ts = tskit.load(file)\n",
    "        #ts = tskit.load(file)\n",
    "        #data_objects = convert_tree_sequence_to_data_object_alpha(ts, get_alpha(file))\n",
    "        nth_scenario = int(file.split(\"_\")[2])\n",
    "        parameter_set = parameters.iloc[nth_scenario]\n",
    "        data_objects = convert_tree_sequence_to_data_object_alpha_r(ts, parameter_set)\n",
    "\n",
    "        if len(data_objects) > 1:\n",
    "        \n",
    "            #mask[population_time <= 10] = False\n",
    "            #mask = torch.tile(torch.Tensor(mask), (len(data_objects), 1))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            dl = DataLoader(data_objects, batch_size=len(data_objects))\n",
    "            for batch in dl:\n",
    "                batch = batch.to(device)\n",
    "                y_hat = model(batch)\n",
    "                y_true = data_objects[0].y.tile(len(data_objects)).reshape(len(data_objects), length).to(device)\n",
    "\n",
    "                loss = criterion(y_hat, y_true) \n",
    "                loss.backward()\n",
    "                loss_all.append(loss.item())\n",
    "                optimizer.step()\n",
    "\n",
    "            if i != 0 and i % 10000 == 0:\n",
    "                loss_all = np.mean(loss_all)\n",
    "                print(f\"loss {loss_all}\")\n",
    "                torch.save(model.state_dict(), \"./alpha_inf_M3/mmc_diffpool_model_alpha_inference_intermediate\" + str(epoch) + \"_\" + str(i) + \".pth\")\n",
    "                os.system(f'echo \"Epoch: {epoch:03d}, Train Loss: {np.mean(loss_all):.4f}\" >> ./alpha_inf_M3/mmc_diffpool_model_alpha_inference_intermediate.txt')\n",
    "                loss_all = []\n",
    "                \n",
    "torch.save(model.state_dict(), \"./alpha_inf_M3/mmc_diffpool_model_alpha_inference_intermediate\" + str(epoch) + \"_\" + str(i) + \".pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "important-checkout",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "floppy-startup",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revised-weight",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lasting-force",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "living-tooth",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parallel-musician",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "harmful-majority",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "altered-matter",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "portuguese-daisy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "realistic-intelligence",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smoking-component",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-running",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-newport",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
