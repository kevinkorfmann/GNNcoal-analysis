{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "smaller-person",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torch_geometric.data import Data\n",
    "#import os\n",
    "#import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "assured-start",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "alpine-mustang",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "\n",
    "sys.path.append(\"/home/ubuntu/GNNcoal/\")\n",
    "#from graphseq_inference.data_utils import *\n",
    "from GNNcoal.models import *\n",
    "#from graphseq_inference.train_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "damaged-crystal",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rm -rf ./20k_dataset/.ipynb_checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "pleasant-suspect",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "arranged-pointer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RMSELoss(yhat,y):\n",
    "    return torch.sqrt(torch.mean((yhat-y)**2))\n",
    "\n",
    "criterion = RMSELoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "unsigned-requirement",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_tree_sequence(ts, num_samples):\n",
    "    return ts.simplify(np.random.choice(range(ts.num_samples),\n",
    "                                 num_samples, replace=False).tolist())\n",
    "\n",
    "def convert_tree_sequence_to_data_object(tree_sequence: tskit.trees.TreeSequence,\n",
    "                                                     parameters: np.ndarray,\n",
    "                                                     num_trees:int = 500,\n",
    "                                                     num_embedding:int = 60, \n",
    "                           ):\n",
    "    \n",
    "    population_size = parameters[\"pop_size_0\":\"pop_size_59\"].tolist() \n",
    "    y = torch.Tensor(population_size)\n",
    "    \n",
    "    tree_sequence = reduce_tree_sequence(tree_sequence, 3)\n",
    "    \n",
    "    max_num_nodes = 2 * tree_sequence.num_samples - 1 \n",
    "    data_objects = []\n",
    "    \n",
    "    #ts = tree_sequence\n",
    "    #ts = msprime.mutate(ts, 1e-8)\n",
    "    #ne = calculate_beta_coal_ne_estimate(ts.num_mutations, ts.sample_size, ts.sequence_length, parameters.model, 1e-8)\n",
    "    \n",
    "    for i, tree in enumerate(tree_sequence.trees()):\n",
    "        if i < num_trees:\n",
    "            \n",
    "            data = from_networkx(nx.Graph(tree.as_dict_of_dicts()))\n",
    "            rename_data_attribute(data, \"branch_length\", \"edge_weight\") \n",
    "            num_nodes = data.num_nodes\n",
    "            data.x = torch.eye(max_num_nodes,num_embedding)\n",
    "            data.x[num_nodes:] = torch.zeros(num_embedding)\n",
    "            data.y = torch.Tensor(torch.log(y))\n",
    "            data.num_nodes = max_num_nodes\n",
    "            \n",
    "            \n",
    "            \n",
    "            #data.edge_weight = data.edge_weight / ne\n",
    "            \n",
    "            data_objects.append(data)\n",
    "            \n",
    "        else: \n",
    "            break\n",
    "\n",
    "        \n",
    "    return data_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alleged-agreement",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = DiffPoolNet(19, 60, 192, 60).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
    "initialize_weights(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "perfect-fluid",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = pd.read_csv(\"20k_seed_0x1337_demographies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proof-trick",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"./20k_dataset/\"\n",
    "\n",
    "files = os.listdir(directory)\n",
    "files = [directory +  file for file in files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hungarian-offering",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artistic-mainstream",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.interpolate import interp1d\n",
    "upper_out_of_bound = lower_out_of_bound = True\n",
    "while upper_out_of_bound or lower_out_of_bound:\n",
    "    steps = 18\n",
    "    x = np.log(get_population_time(time_rate=0.1, num_time_windows=steps, tmax=10_000_000).tolist())\n",
    "    y = np.log(sample_population_size(10_000, 10_000_000, steps))\n",
    "    xnew = np.linspace(x[0], x[-1], num=10000, endpoint=True)\n",
    "    f_cubic = interp1d(x, y, kind='cubic')\n",
    "    ynew = f_cubic(xnew)\n",
    "    upper_out_of_bound = np.sum(np.exp(ynew) > 10_000_000) > 0\n",
    "    lower_out_of_bound = np.sum(np.exp(ynew) < 10_000) > 0\n",
    "    \n",
    "x_sample = xnew[np.linspace(10, 9999, 60).astype(int)]\n",
    "y_sample = ynew[np.linspace(10, 9999, 60).astype(int)]\n",
    "\n",
    "\n",
    "population_time = np.exp(x_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitted-pharmaceutical",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "continental-cleveland",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occupied-charge",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plain-increase",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loaded-christmas",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "current-roads",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏                              | 10001/2000000 [45:29<136:31:36,  4.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 1.029905827544085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▎                            | 20001/2000000 [1:31:37<201:42:31,  2.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.891035649113615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▍                            | 30001/2000000 [2:17:03<181:22:16,  3.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.8317873635160348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▌                            | 40001/2000000 [3:04:15<113:06:33,  4.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.8104778996700935\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▋                            | 50001/2000000 [3:50:11<122:25:49,  4.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.7937906841228197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▊                            | 60002/2000000 [4:38:06<151:42:28,  3.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.7846791026897555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|█                            | 70001/2000000 [5:26:36<143:55:38,  3.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.7734038183728388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|█▏                           | 80001/2000000 [6:15:32<122:13:23,  4.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.7688198455148386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|█▎                           | 90002/2000000 [7:03:06<134:30:47,  3.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.754977219590662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|█▍                          | 100001/2000000 [7:51:15<154:46:33,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.7549446300253679\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|█▌                          | 110001/2000000 [8:47:31<184:54:50,  2.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.7484181548188151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|█▋                          | 120001/2000000 [9:42:02<196:11:40,  2.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.7470141353676932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|█▊                         | 130001/2000000 [10:38:37<234:58:25,  2.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.7436964030947174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|█▉                         | 140001/2000000 [11:35:02<167:17:22,  3.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.7348751018806696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|██                         | 150001/2000000 [12:30:23<150:54:20,  3.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss 0.7381129761972186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|██                         | 152896/2000000 [12:45:27<219:26:57,  2.34it/s]"
     ]
    }
   ],
   "source": [
    "length = 60\n",
    "loss_all = []\n",
    "\n",
    "for epoch in range(0, 2):\n",
    "    np.random.shuffle(files)\n",
    "    for i in tqdm(range(0, len(files))):\n",
    "\n",
    "        file = files[i]\n",
    "        ts, mask = torch.load(file)\n",
    "        \n",
    "        nth_scenario = int(file.split(\"_\")[2])\n",
    "        \n",
    "\n",
    "        parameter_set = parameters.iloc[nth_scenario]\n",
    "        data_objects = convert_tree_sequence_to_data_object(ts, parameter_set)\n",
    "\n",
    "        if len(data_objects) > 1:\n",
    "        \n",
    "            mask[population_time <= 10] = False\n",
    "            mask = torch.tile(torch.Tensor(mask), (len(data_objects), 1))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            dl = DataLoader(data_objects, batch_size=len(data_objects))\n",
    "            for batch in dl:\n",
    "                batch = batch.to(device)\n",
    "                y_hat = model(batch)\n",
    "                y_true = data_objects[0].y.tile(len(data_objects)).reshape(len(data_objects), length).to(device)\n",
    "\n",
    "                mask = mask.to(device)\n",
    "                mask = mask.bool()\n",
    "                y_true[~mask] = 0\n",
    "                y_hat[~mask] = 0\n",
    "                loss = criterion(y_hat, y_true) \n",
    "                loss.backward()\n",
    "                loss_all.append(loss.item())\n",
    "                optimizer.step()\n",
    "\n",
    "            #if i != 0 and i % 100000 == 0:\n",
    "            if i != 0 and i % 10000 == 0:\n",
    "\n",
    "                loss_all = np.mean(loss_all)\n",
    "                print(f\"loss {loss_all}\")\n",
    "                if i != 0 and i % 100000 == 0: torch.save(model.state_dict(), \"./large_models_demo_nescaling_M3/mmc_diffpool_model_demography_inference_intermediate\" + str(epoch) + \"_\" + str(i) + \".pth\")\n",
    "                os.system(f'echo \"Epoch: {epoch:03d}, Train Loss: {np.mean(loss_all):.4f}\" >> ./large_models_demo_nescaling_M3/mmc_diffpool_model_demography_inference_intermediate.txt')\n",
    "                loss_all = []\n",
    "                \n",
    "    torch.save(model.state_dict(), \"./trained_models/demography_models_M3/mmc_diffpool_model_demography_inference\" + str(epoch) + \"_\" + str(i) + \".pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laden-nurse",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
